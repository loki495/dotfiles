# MCPHost Configuration File
# All command-line flags can be configured here
# This demonstrates the simplified local/remote/builtin server configuration

# MCP Servers configuration
# Add your MCP servers here
# Examples for different server types:
mcpServers:
#   # Local MCP servers - run commands locally via stdio transport
#   filesystem-local:
#     type: "local"
#     command: ["npx", "@modelcontextprotocol/server-filesystem", "/tmp"]
#     environment:
#       DEBUG: "true"
#       LOG_LEVEL: "info"
#   
#   sqlite:
#     type: "local" 
#     command: ["uvx", "mcp-server-sqlite", "--db-path", "/tmp/example.db"]
#     environment:
#       SQLITE_DEBUG: "1"
#   
   laravel-boost:
     type: "local"
     command: [ "php", "${env://PWD}/artisan", "boost:mcp" ]

   # Builtin MCP servers - run in-process for optimal performance
   filesystem-builtin:
     type: "builtin"
     name: "fs"
     options:
       allowed_directories: ["/tmp", "/home/andres"]
     allowedTools: ["read_file", "write_file", "list_directory"]
#   
   # Minimal builtin server - defaults to current working directory
   filesystem-cwd:
     type: "builtin"
     name: "fs"
#   
   # Bash server for shell commands
   bash:
     type: "builtin"
     name: "bash"
   
   # Todo server for task management
   todo:
     type: "builtin"
     name: "todo"
#   
   # Fetch server for web content
   fetch:
     type: "builtin"
     name: "fetch"
#   
#   # Remote MCP servers - connect via StreamableHTTP transport
#   # Optional 'headers' field can be used for authentication and custom headers
#   websearch:
#     type: "remote"
#     url: "https://api.example.com/mcp"
#   
#   weather:
#     type: "remote"
#     url: "https://weather-mcp.example.com"
#   
#   # Legacy format still supported for backward compatibility:
#   # legacy-server:
#   #   command: npx
#   #   args: ["@modelcontextprotocol/server-filesystem", "/path"]
#   #   env:
#   #     MY_VAR: "value"

#mcpServers:

# Application settings (all optional)
# model: "anthropic:claude-sonnet-4-20250514"  # Default model to use
# max-steps: 10                                # Maximum agent steps (0 for unlimited)
# debug: false                                 # Enable debug logging
# system-prompt: "/path/to/system-prompt.txt" # System prompt text file

# Model generation parameters (all optional)
# max-tokens: 4096                             # Maximum tokens in response
# temperature: 0.7                             # Randomness (0.0-1.0)
# top-p: 0.95                                  # Nucleus sampling (0.0-1.0)
# top-k: 40                                    # Top K sampling
# stop-sequences: ["Human:", "Assistant:"]     # Custom stop sequences

# API Configuration (can also use environment variables)
# provider-api-key: "your-api-key"         # API key for OpenAI, Anthropic, or Google
# provider-url: "https://api.openai.com/v1" # Base URL for OpenAI, Anthropic, or Ollama
#model: "ollama:qwen3:4b"
#stream: true
#cpu: true # CPU inference with quantization
#quantization: q4_k_m   # 4-bit quantization to fit 16GB RAM
#max_tokens: 2048
#temperature: 0.2

model: "ollama:gemma:7b"
#pu: true
ax_tokens: 2048
emperature: 0.2
